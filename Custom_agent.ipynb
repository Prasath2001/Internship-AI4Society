{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8e9a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Any,Optional,List\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "import textworld\n",
    "import textworld.gym\n",
    "from textworld import EnvInfos\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b28ce5",
   "metadata": {},
   "source": [
    "## Random Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88171026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(textworld.gym.Agent):\n",
    "    def __init__(self,seed=10):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "    \n",
    "    @property\n",
    "    def infos_to_request(self) -> textworld.EnvInfos:\n",
    "        return textworld.EnvInfos(admissible_commands = True)\n",
    "    \n",
    "    def act(self,obs:str,score:int,done:bool,infos:Mapping[str,Any]) -> str:\n",
    "        return self.rng.choice(infos['admissible_commands'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4b813",
   "metadata": {},
   "source": [
    "## Play Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d5f70bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import gym\n",
    "\n",
    "def play(agent,path,max_step=100,nb_episodes=10,verbose=True):\n",
    "    infos_to_request = agent.infos_to_request\n",
    "    infos_to_request.max_score = True\n",
    "    \n",
    "    gamefiles = [path]\n",
    "    if os.path.isdir(path):\n",
    "        gamefiles = glob(os.path.join(path,'*.ulx'))\n",
    "    \n",
    "    env_id = textworld.gym.register_games(gamefiles,\n",
    "                                          request_infos=infos_to_request,\n",
    "                                          max_episode_steps=max_step)\n",
    "    env = gym.make(env_id)\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(os.path.dirname(path),end=\"\")\n",
    "        else:\n",
    "            print(os.path.basename(path),end=\"\")\n",
    "    \n",
    "    avg_moves, avg_scores, avg_norm_scores = [],[],[]\n",
    "    for no_episode in range(nb_episodes):\n",
    "        obs, infos = env.reset() # To start new Episode\n",
    "        score = 0\n",
    "        done = False\n",
    "        nb_moves = 0\n",
    "        while not done:\n",
    "            command = agent.act(obs,score,done,infos)\n",
    "            obs,score,done,info = env.step(command)\n",
    "            nb_moves+=1\n",
    "            \n",
    "        agent.act(obs,score,done,infos)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\".\",end=\"\")\n",
    "        avg_moves.append(nb_moves)\n",
    "        avg_scores.append(score)\n",
    "        avg_norm_scores.append(score/infos[\"max_score\"])\n",
    "    \n",
    "    env.close()\n",
    "    msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
    "    \n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(msg.format(np.mean(avg_moves),np.mean(avg_scores),1))\n",
    "        else:\n",
    "            print(msg.format(np.mean(avg_moves),np.mean(avg_scores),infos[\"max_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42137a",
   "metadata": {},
   "source": [
    "## Testing the Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4878844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewardsDense_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  1.0 / 10.\n",
      "rewardsBalanced_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  0.0 / 4.\n",
      "rewardsSparse_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  0.0 / 1.\n"
     ]
    }
   ],
   "source": [
    "play(RandomAgent(),\"./games/rewardsDense_goalDetailed.ulx\")\n",
    "play(RandomAgent(), \"./games/rewardsBalanced_goalDetailed.ulx\")\n",
    "play(RandomAgent(), \"./games/rewardsSparse_goalDetailed.ulx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6198b1",
   "metadata": {},
   "source": [
    "## Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0b6b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommandScorer(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super(CommandScorer,self).__init__()\n",
    "        torch.manual_seed(1)\n",
    "        self.embedding = nn.Embedding(input_size,hidden_size)\n",
    "        self.encoder_gru = nn.GRU(hidden_size,hidden_size)\n",
    "        self.cmd_encoder_gru = nn.GRU(hidden_size,hidden_size)\n",
    "        self.state_gru = nn.GRU(hidden_size,hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "        self.critic = nn.Linear(hidden_size,1)\n",
    "        self.att_cmd = nn.Linear(hidden_size*2,1)\n",
    "    \n",
    "    def forward(self,obs,commands,*kwargs):\n",
    "        input_length = obs.size(0)\n",
    "        batch_size = obs.size(1)\n",
    "        nb_cmds = commands.size(1)\n",
    "        \n",
    "        embedded = self.embedding(obs)\n",
    "        encoder_output, encoder_hidden = self.encoder_gru(embedded)\n",
    "        state_output, state_hidden = self.state_gru(encoder_hidden,self.state_hidden)\n",
    "        self.state_hidden = state_hidden\n",
    "        value = self.critic(state_output)\n",
    "        \n",
    "        #Attention network over the commands\n",
    "        cmds_embedding = self.embedding.forward(commands)\n",
    "        _,cmds_encoding_last_states = self.cmd_encoder_gru.forward(cmds_embedding) # 1 x cmds x hidden \n",
    "        \n",
    "        #Same observed state for all commands\n",
    "        cmd_selector_input = torch.stack([state_hidden]*nb_cmds,2) # 1 x batch x cmds x hidden\n",
    "        \n",
    "        #Same command choices for whole batch\n",
    "        cmds_encoding_last_states = torch.stack([cmds_encoding_last_states]*batch_size,1) # 1 x batch x cmds x hidden\n",
    "        \n",
    "        # Concatenate the observed state and command encodings\n",
    "        cmd_selector_input = torch.cat([cmd_selector_input,cmds_encoding_last_states],dim=-1)\n",
    "        \n",
    "        #Compute on score per command\n",
    "        scores = F.relu(self.att_cmd(cmd_selector_input)).squeeze(-1) # 1x batch x cmds\n",
    "        \n",
    "        probs = F.softmax(scores,dim=2) # 1 x batch x cmds\n",
    "        index = probs[0].multinomial(num_samples=1).unsqueeze(0) # 1 x batch x index\n",
    "        return scores,index,value\n",
    "    \n",
    "    def reset_hidden(self,batch_size):\n",
    "        self.state_hidden = torch.zeros(1,batch_size,self.hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb42d6",
   "metadata": {},
   "source": [
    "## Neural Agent (with PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18d11601",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 1000\n",
    "UPDATE_FREQUENCY = 10\n",
    "LOG_FREQUENCY = 1000\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02498180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAgent:\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._initialized = False\n",
    "        self._episode_has_started = False\n",
    "        self.id2word = [\"<PAD>\",\"<UNK>\"]\n",
    "        self.word2id = {w:i for i,w in enumerate(self.id2word)}\n",
    "        self.MAX_VOCAB_SIZE = 1000\n",
    "        self.UPDATE_FREQUENCY = 10\n",
    "        self.LOG_FREQUENCY = 1000\n",
    "        self.GAMMA = 0.9\n",
    "        self.model = CommandScorer(input_size = MAX_VOCAB_SIZE,hidden_size = 128)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),0.00003)\n",
    "        \n",
    "        self.mode = \"test\"\n",
    "    \n",
    "    def train(self):\n",
    "        self.mode = \"train\"\n",
    "        self.stats = {\"max\":defaultdict(list),\"mean\":defaultdict(list)}\n",
    "        self.transitions = []\n",
    "        self.model.reset_hidden(1)\n",
    "        self.last_score = 0 \n",
    "        self.no_train_step = 0\n",
    "        \n",
    "    def test(self):\n",
    "        self.mode = \"test\"\n",
    "        self.model.reset_hidden(1)\n",
    "    \n",
    "    @property\n",
    "    def infos_to_request(self) -> EnvInfos:\n",
    "        return EnvInfos(description=True,inventory=True,admissible_commands=True,won=True,lost=True)\n",
    "    \n",
    "    def get_word_id(self,word):\n",
    "        if word not in self.word2id:\n",
    "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
    "                return self.word2id[\"<UNK>\"]\n",
    "            \n",
    "            self.id2word.append(word)\n",
    "            self.word2id[word] = len(self.word2id)\n",
    "        return self.word2id[word]\n",
    "    \n",
    "    def _tokenize(self,text):\n",
    "        # To strip out all non-alphabetic characters\n",
    "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
    "        word_ids = list(map(self.get_word_id,text.split()))\n",
    "        return word_ids\n",
    "    \n",
    "    def _process(self,texts):\n",
    "        texts = list(map(self._tokenize,texts))\n",
    "        max_len = max(len(l) for l in texts)\n",
    "        padded = np.ones((len(texts),max_len)) * self.word2id[\"<PAD>\"]\n",
    "        \n",
    "        for i,text in enumerate(texts):\n",
    "            padded[i,:len(text)] = text\n",
    "        \n",
    "        padded_tensor = torch.from_numpy(padded).type(torch.long).to(device)\n",
    "        padded_tensor = padded_tensor.permute(1,0) # Batch x Seq => Seq x Batch\n",
    "        return padded_tensor\n",
    "        \n",
    "    def _discount_rewards(self,last_values):\n",
    "        returns, advantages = [], []\n",
    "        R = last_values.data\n",
    "        for t in reversed(range(len(self.transitions))):\n",
    "            rewards, _, _, values = self.transitions[t]\n",
    "            R = rewards + self.GAMMA * R\n",
    "            adv = R - values\n",
    "            returns.append(R)\n",
    "            advantages.append(adv)\n",
    "        \n",
    "        return returns[::-1], advantages[::-1]\n",
    "    \n",
    "    def act(self,obs:str,score:int,done:bool,infos : Mapping[str,Any]) -> Optional[str]:\n",
    "        \n",
    "        # Build Agents observation : feedback + look + inventory\n",
    "        input_ = \"{}\\n{}\\n{}\".format(obs,infos['description'],infos['inventory'])\n",
    "        \n",
    "        #Tokenize and pad the input and the commands to choose from\n",
    "        input_tensor = self._process([input_])\n",
    "        commands_tensor = self._process(infos[\"admissible_commands\"])\n",
    "        \n",
    "        # Get our next action and value prediction\n",
    "        outputs, indexes, values = self.model(input_tensor,commands_tensor)\n",
    "        action = infos[\"admissible_commands\"][indexes[0]]\n",
    "        \n",
    "        if self.mode == \"test\":\n",
    "            if done:\n",
    "                self.model.reset_hidden(1)\n",
    "            return action\n",
    "        \n",
    "        self.no_train_step += 1\n",
    "        \n",
    "        if self.transitions:\n",
    "            reward = score - self.last_score # Reward is the gain or loss in the game.\n",
    "            self.last_score = score\n",
    "            if infos[\"won\"]:\n",
    "                reward += 100\n",
    "            if infos[\"lost\"]:\n",
    "                reward -= 100\n",
    "                \n",
    "            self.transitions[-1][0] = reward  # Update reward information.\n",
    "        \n",
    "        self.stats[\"max\"][\"score\"].append(score)\n",
    "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
    "            # Update model\n",
    "            returns, advantages = self._discount_rewards(values)\n",
    "            \n",
    "            loss = 0\n",
    "            for transition, ret, advantage in zip(self.transitions, returns, advantages):\n",
    "                reward, indexes_, outputs_, values_ = transition\n",
    "                \n",
    "                advantage        = advantage.detach() # Block gradients flow here.\n",
    "                probs            = F.softmax(outputs_, dim=2)\n",
    "                log_probs        = torch.log(probs)\n",
    "                log_action_probs = log_probs.gather(2, indexes_)\n",
    "                policy_loss      = (-log_action_probs * advantage).sum()\n",
    "                value_loss       = (.5 * (values_ - ret) ** 2.).sum()\n",
    "                entropy     = (-probs * log_probs).sum()\n",
    "                loss += policy_loss + 0.5 * value_loss - 0.1 * entropy\n",
    "                \n",
    "                self.stats[\"mean\"][\"reward\"].append(reward)\n",
    "                self.stats[\"mean\"][\"policy\"].append(policy_loss.item())\n",
    "                self.stats[\"mean\"][\"value\"].append(value_loss.item())\n",
    "                self.stats[\"mean\"][\"entropy\"].append(entropy.item())\n",
    "                self.stats[\"mean\"][\"confidence\"].append(torch.exp(log_action_probs).item())\n",
    "            \n",
    "            if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
    "                msg = \"{}. \".format(self.no_train_step)\n",
    "                msg += \"  \".join(\"{}: {:.3f}\".format(k, np.mean(v)) for k, v in self.stats[\"mean\"].items())\n",
    "                msg += \"  \" + \"  \".join(\"{}: {}\".format(k, np.max(v)) for k, v in self.stats[\"max\"].items())\n",
    "                msg += \"  vocab: {}\".format(len(self.id2word))\n",
    "                print(msg)\n",
    "                self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "            \n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(),40)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            self.transitions = []\n",
    "            self.model.reset_hidden(1)\n",
    "        else:\n",
    "            self.transitions.append([None, indexes, outputs, values])\n",
    "            \n",
    "        if done:\n",
    "            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744c5a41",
   "metadata": {},
   "source": [
    "## Training the neural agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bca25a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewardsDense_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  1.0 / 10.\n"
     ]
    }
   ],
   "source": [
    "agent = NeuralAgent()\n",
    "play(agent, \"./games/rewardsDense_goalDetailed.ulx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26ea3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "1000. reward: 0.011  policy: 0.029  value: 0.014  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "2000. reward: 0.011  policy: 0.015  value: 0.016  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "3000. reward: 0.011  policy: 0.015  value: 0.017  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "4000. reward: 0.011  policy: 0.013  value: 0.018  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "5000. reward: 0.011  policy: -0.014  value: 0.016  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "6000. reward: 0.011  policy: 0.016  value: 0.018  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "7000. reward: 0.011  policy: -0.005  value: 0.015  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "8000. reward: 0.011  policy: 0.015  value: 0.015  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "9000. reward: 0.011  policy: 0.016  value: 0.017  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "10000. reward: 0.010  policy: -0.008  value: 0.015  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "11000. reward: 0.011  policy: -0.005  value: 0.015  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "12000. reward: 0.011  policy: 0.015  value: 0.018  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "13000. reward: 0.011  policy: -0.008  value: 0.017  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "14000. reward: 0.011  policy: 0.007  value: 0.017  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "15000. reward: 0.011  policy: -0.003  value: 0.016  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "16000. reward: 0.011  policy: 0.010  value: 0.015  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "17000. reward: 0.011  policy: -0.010  value: 0.013  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "18000. reward: 0.011  policy: 0.027  value: 0.016  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "19000. reward: 0.011  policy: -0.009  value: 0.013  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "20000. reward: 0.010  policy: 0.004  value: 0.015  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "21000. reward: 0.011  policy: 0.007  value: 0.018  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "22000. reward: 0.011  policy: -0.004  value: 0.017  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "23000. reward: 0.011  policy: 0.002  value: 0.014  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "24000. reward: 0.011  policy: -0.013  value: 0.012  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "25000. reward: 0.011  policy: 0.014  value: 0.015  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "26000. reward: 0.011  policy: 0.004  value: 0.016  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "27000. reward: 0.011  policy: -0.011  value: 0.015  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "28000. reward: 0.011  policy: 0.005  value: 0.014  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "29000. reward: 0.011  policy: 0.020  value: 0.018  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "30000. reward: 0.011  policy: 0.012  value: 0.021  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "31000. reward: 0.010  policy: -0.019  value: 0.016  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "32000. reward: 0.011  policy: -0.004  value: 0.016  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "33000. reward: 0.011  policy: 0.005  value: 0.017  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "34000. reward: 0.011  policy: 0.004  value: 0.015  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "35000. reward: 0.011  policy: -0.004  value: 0.013  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "36000. reward: 0.011  policy: 0.003  value: 0.016  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "37000. reward: 0.011  policy: 0.010  value: 0.017  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "38000. reward: 0.011  policy: -0.002  value: 0.018  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "39000. reward: 0.011  policy: 0.010  value: 0.018  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "40000. reward: 0.010  policy: -0.017  value: 0.013  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "41000. reward: 0.011  policy: 0.007  value: 0.017  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "42000. reward: 0.011  policy: 0.002  value: 0.017  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "43000. reward: 0.011  policy: -0.007  value: 0.014  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "44000. reward: 0.011  policy: 0.001  value: 0.013  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "45000. reward: 0.011  policy: 0.019  value: 0.017  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "46000. reward: 0.011  policy: -0.004  value: 0.018  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "47000. reward: 0.011  policy: -0.009  value: 0.015  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "48000. reward: 0.011  policy: 0.012  value: 0.014  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "49000. reward: 0.010  policy: -0.004  value: 0.014  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "50000. reward: 0.011  policy: 0.001  value: 0.016  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "Trained in 1818.42 secs\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "agent = NeuralAgent()\n",
    "\n",
    "print(\"Training\")\n",
    "agent.train()\n",
    "starttime = time()\n",
    "play(agent,\"./games/rewardsDense_goalDetailed.ulx\", nb_episodes=500, verbose=False)\n",
    "print(\"Trained in {:.2f} secs\".format(time() - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7cc9bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewardsDense_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  1.0 / 10.\n"
     ]
    }
   ],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "agent.test()\n",
    "play(agent, \"./games/rewardsDense_goalDetailed.ulx\")  # Dense rewards game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9111b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
