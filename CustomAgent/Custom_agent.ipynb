{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8e9a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Any,Optional,List\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "import textworld\n",
    "import textworld.gym\n",
    "from textworld import EnvInfos\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b28ce5",
   "metadata": {},
   "source": [
    "## Random Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88171026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(textworld.gym.Agent):\n",
    "    def __init__(self,seed=10):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "    \n",
    "    @property\n",
    "    def infos_to_request(self) -> textworld.EnvInfos:\n",
    "        return textworld.EnvInfos(admissible_commands = True)\n",
    "    \n",
    "    def act(self,obs:str,score:int,done:bool,infos:Mapping[str,Any]) -> str:\n",
    "        return self.rng.choice(infos['admissible_commands'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4b813",
   "metadata": {},
   "source": [
    "## Play Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d5f70bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import gym\n",
    "\n",
    "def play(agent,path,max_step=100,nb_episodes=10,verbose=True):\n",
    "    infos_to_request = agent.infos_to_request\n",
    "    infos_to_request.max_score = True\n",
    "    \n",
    "    gamefiles = [path]\n",
    "    if os.path.isdir(path):\n",
    "        gamefiles = glob(os.path.join(path,'*.ulx'))\n",
    "    \n",
    "    env_id = textworld.gym.register_games(gamefiles,\n",
    "                                          request_infos=infos_to_request,\n",
    "                                          max_episode_steps=max_step)\n",
    "    env = gym.make(env_id)\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(os.path.dirname(path),end=\"\")\n",
    "        else:\n",
    "            print(os.path.basename(path),end=\"\")\n",
    "    \n",
    "    avg_moves, avg_scores, avg_norm_scores = [],[],[]\n",
    "    for no_episode in range(nb_episodes):\n",
    "        obs, infos = env.reset() # To start new Episode\n",
    "        score = 0\n",
    "        done = False\n",
    "        nb_moves = 0\n",
    "        while not done:\n",
    "            command = agent.act(obs,score,done,infos)\n",
    "            obs,score,done,info = env.step(command)\n",
    "            nb_moves+=1\n",
    "            \n",
    "        agent.act(obs,score,done,infos)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\".\",end=\"\")\n",
    "        avg_moves.append(nb_moves)\n",
    "        avg_scores.append(score)\n",
    "        avg_norm_scores.append(score/infos[\"max_score\"])\n",
    "    \n",
    "    env.close()\n",
    "    msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
    "    \n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(msg.format(np.mean(avg_moves),np.mean(avg_norm_scores),1))\n",
    "        else:\n",
    "            print(msg.format(np.mean(avg_moves),np.mean(avg_scores),infos[\"max_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42137a",
   "metadata": {},
   "source": [
    "## Testing the Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4878844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewardsDense_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  1.0 / 10.\n",
      "rewardsBalanced_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  0.0 / 4.\n",
      "rewardsSparse_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  0.0 / 1.\n"
     ]
    }
   ],
   "source": [
    "play(RandomAgent(),\"./games/rewardsDense_goalDetailed.ulx\")\n",
    "play(RandomAgent(), \"./games/rewardsBalanced_goalDetailed.ulx\")\n",
    "play(RandomAgent(), \"./games/rewardsSparse_goalDetailed.ulx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6198b1",
   "metadata": {},
   "source": [
    "## Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0b6b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommandScorer(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super(CommandScorer,self).__init__()\n",
    "        torch.manual_seed(1)\n",
    "        self.embedding = nn.Embedding(input_size,hidden_size)\n",
    "        self.encoder_gru = nn.GRU(hidden_size,hidden_size)\n",
    "        self.cmd_encoder_gru = nn.GRU(hidden_size,hidden_size)\n",
    "        self.state_gru = nn.GRU(hidden_size,hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "        self.critic = nn.Linear(hidden_size,1)\n",
    "        self.att_cmd = nn.Linear(hidden_size*2,1)\n",
    "    \n",
    "    def forward(self,obs,commands,*kwargs):\n",
    "        input_length = obs.size(0)\n",
    "        batch_size = obs.size(1)\n",
    "        nb_cmds = commands.size(1)\n",
    "        \n",
    "        embedded = self.embedding(obs)\n",
    "        encoder_output, encoder_hidden = self.encoder_gru(embedded)\n",
    "        state_output, state_hidden = self.state_gru(encoder_hidden,self.state_hidden)\n",
    "        self.state_hidden = state_hidden\n",
    "        value = self.critic(state_output)\n",
    "        \n",
    "        #Attention network over the commands\n",
    "        cmds_embedding = self.embedding.forward(commands)\n",
    "        _,cmds_encoding_last_states = self.cmd_encoder_gru.forward(cmds_embedding) # 1 x cmds x hidden \n",
    "        \n",
    "        #Same observed state for all commands\n",
    "        cmd_selector_input = torch.stack([state_hidden]*nb_cmds,2) # 1 x batch x cmds x hidden\n",
    "        \n",
    "        #Same command choices for whole batch\n",
    "        cmds_encoding_last_states = torch.stack([cmds_encoding_last_states]*batch_size,1) # 1 x batch x cmds x hidden\n",
    "        \n",
    "        # Concatenate the observed state and command encodings\n",
    "        cmd_selector_input = torch.cat([cmd_selector_input,cmds_encoding_last_states],dim=-1)\n",
    "        \n",
    "        #Compute on score per command\n",
    "        scores = F.relu(self.att_cmd(cmd_selector_input)).squeeze(-1) # 1x batch x cmds\n",
    "        \n",
    "        probs = F.softmax(scores,dim=2) # 1 x batch x cmds\n",
    "        index = probs[0].multinomial(num_samples=1).unsqueeze(0) # 1 x batch x index\n",
    "        return scores,index,value\n",
    "    \n",
    "    def reset_hidden(self,batch_size):\n",
    "        self.state_hidden = torch.zeros(1,batch_size,self.hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb42d6",
   "metadata": {},
   "source": [
    "## Neural Agent (with PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05b62b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 1000\n",
    "UPDATE_FREQUENCY = 10\n",
    "LOG_FREQUENCY = 1000\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02498180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAgent:\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._initialized = False\n",
    "        self._episode_has_started = False\n",
    "        self.id2word = [\"<PAD>\",\"<UNK>\"]\n",
    "        self.word2id = {w:i for i,w in enumerate(self.id2word)}\n",
    "        self.MAX_VOCAB_SIZE = 1000\n",
    "        self.UPDATE_FREQUENCY = 10\n",
    "        self.LOG_FREQUENCY = 1000\n",
    "        self.GAMMA = 0.9\n",
    "        self.model = CommandScorer(input_size = MAX_VOCAB_SIZE,hidden_size = 128)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),0.00003)\n",
    "        \n",
    "        self.mode = \"test\"\n",
    "    \n",
    "    def train(self):\n",
    "        self.mode = \"train\"\n",
    "        self.stats = {\"max\":defaultdict(list),\"mean\":defaultdict(list)}\n",
    "        self.transitions = []\n",
    "        self.model.reset_hidden(1)\n",
    "        self.last_score = 0 \n",
    "        self.no_train_step = 0\n",
    "        \n",
    "    def test(self):\n",
    "        self.mode = \"test\"\n",
    "        self.model.reset_hidden(1)\n",
    "    \n",
    "    @property\n",
    "    def infos_to_request(self) -> EnvInfos:\n",
    "        return EnvInfos(description=True,inventory=True,admissible_commands=True,won=True,lost=True)\n",
    "    \n",
    "    def get_word_id(self,word):\n",
    "        if word not in self.word2id:\n",
    "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
    "                return self.word2id[\"<UNK>\"]\n",
    "            \n",
    "            self.id2word.append(word)\n",
    "            self.word2id[word] = len(self.word2id)\n",
    "        return self.word2id[word]\n",
    "    \n",
    "    def _tokenize(self,text):\n",
    "        # To strip out all non-alphabetic characters\n",
    "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
    "        word_ids = list(map(self.get_word_id,text.split()))\n",
    "        return word_ids\n",
    "    \n",
    "    def _process(self,texts):\n",
    "        texts = list(map(self._tokenize,texts))\n",
    "        max_len = max(len(l) for l in texts)\n",
    "        padded = np.ones((len(texts),max_len)) * self.word2id[\"<PAD>\"]\n",
    "        \n",
    "        for i,text in enumerate(texts):\n",
    "            padded[i,:len(text)] = text\n",
    "        \n",
    "        padded_tensor = torch.from_numpy(padded).type(torch.long).to(device)\n",
    "        padded_tensor = padded_tensor.permute(1,0) # Batch x Seq => Seq x Batch\n",
    "        return padded_tensor\n",
    "        \n",
    "    def _discount_rewards(self,last_values):\n",
    "        returns, advantages = [], []\n",
    "        R = last_values.data\n",
    "        for t in reversed(range(len(self.transitions))):\n",
    "            rewards, _, _, values = self.transitions[t]\n",
    "            R = rewards + self.GAMMA * R\n",
    "            adv = R - values\n",
    "            returns.append(R)\n",
    "            advantages.append(adv)\n",
    "        \n",
    "        return returns[::-1], advantages[::-1]\n",
    "    \n",
    "    def act(self,obs:str,score:int,done:bool,infos : Mapping[str,Any]) -> Optional[str]:\n",
    "        \n",
    "        # Build Agents observation : feedback + look + inventory\n",
    "        input_ = \"{}\\n{}\\n{}\".format(obs,infos['description'],infos['inventory'])\n",
    "        \n",
    "        #Tokenize and pad the input and the commands to choose from\n",
    "        input_tensor = self._process([input_])\n",
    "        commands_tensor = self._process(infos[\"admissible_commands\"])\n",
    "        \n",
    "        # Get our next action and value prediction\n",
    "        outputs, indexes, values = self.model(input_tensor,commands_tensor)\n",
    "        action = infos[\"admissible_commands\"][indexes[0]]\n",
    "        \n",
    "        if self.mode == \"test\":\n",
    "            if done:\n",
    "                self.model.reset_hidden(1)\n",
    "            return action\n",
    "        \n",
    "        self.no_train_step += 1\n",
    "        \n",
    "        if self.transitions:\n",
    "            reward = score - self.last_score # Reward is the gain or loss in the game.\n",
    "            self.last_score = score\n",
    "            if infos[\"won\"]:\n",
    "                reward += 100\n",
    "            if infos[\"lost\"]:\n",
    "                reward -= 100\n",
    "                \n",
    "            self.transitions[-1][0] = reward  # Update reward information.\n",
    "        \n",
    "        self.stats[\"max\"][\"score\"].append(score)\n",
    "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
    "            # Update model\n",
    "            returns, advantages = self._discount_rewards(values)\n",
    "            \n",
    "            loss = 0\n",
    "            for transition, ret, advantage in zip(self.transitions, returns, advantages):\n",
    "                reward, indexes_, outputs_, values_ = transition\n",
    "                \n",
    "                advantage        = advantage.detach() # Block gradients flow here.\n",
    "                probs            = F.softmax(outputs_, dim=2)\n",
    "                log_probs        = torch.log(probs)\n",
    "                log_action_probs = log_probs.gather(2, indexes_)\n",
    "                policy_loss      = (-log_action_probs * advantage).sum()\n",
    "                value_loss       = (.5 * (values_ - ret) ** 2.).sum()\n",
    "                entropy     = (-probs * log_probs).sum()\n",
    "                loss += policy_loss + 0.5 * value_loss - 0.1 * entropy\n",
    "                \n",
    "                self.stats[\"mean\"][\"reward\"].append(reward)\n",
    "                self.stats[\"mean\"][\"policy\"].append(policy_loss.item())\n",
    "                self.stats[\"mean\"][\"value\"].append(value_loss.item())\n",
    "                self.stats[\"mean\"][\"entropy\"].append(entropy.item())\n",
    "                self.stats[\"mean\"][\"confidence\"].append(torch.exp(log_action_probs).item())\n",
    "            \n",
    "            if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
    "                msg = \"{}. \".format(self.no_train_step)\n",
    "                msg += \"  \".join(\"{}: {:.3f}\".format(k, np.mean(v)) for k, v in self.stats[\"mean\"].items())\n",
    "                msg += \"  \" + \"  \".join(\"{}: {}\".format(k, np.max(v)) for k, v in self.stats[\"max\"].items())\n",
    "                msg += \"  vocab: {}\".format(len(self.id2word))\n",
    "                print(msg)\n",
    "                self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "            \n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(),40)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            self.transitions = []\n",
    "            self.model.reset_hidden(1)\n",
    "        else:\n",
    "            self.transitions.append([None, indexes, outputs, values])\n",
    "            \n",
    "        if done:\n",
    "            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744c5a41",
   "metadata": {},
   "source": [
    "## Training the neural agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bca25a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewardsDense_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  1.0 / 10.\n"
     ]
    }
   ],
   "source": [
    "agent = NeuralAgent()\n",
    "play(agent, \"./games/rewardsDense_goalDetailed.ulx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26ea3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "100. reward: 0.011  policy: 0.060  value: 0.011  entropy: 2.079  confidence: 0.126  score: 1  vocab: 145\n",
      "200. reward: 0.011  policy: 0.066  value: 0.017  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "300. reward: 0.011  policy: 0.084  value: 0.022  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "400. reward: 0.011  policy: 0.028  value: 0.017  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "500. reward: 0.011  policy: 0.033  value: 0.019  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "600. reward: 0.011  policy: -0.018  value: 0.011  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "700. reward: 0.011  policy: -0.012  value: 0.011  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "800. reward: 0.011  policy: -0.022  value: 0.006  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "900. reward: 0.011  policy: 0.008  value: 0.010  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "1000. reward: 0.011  policy: 0.059  value: 0.019  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "1100. reward: 0.011  policy: 0.034  value: 0.016  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "1200. reward: 0.011  policy: 0.053  value: 0.020  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "1300. reward: 0.011  policy: 0.072  value: 0.024  entropy: 2.078  confidence: 0.125  score: 1  vocab: 145\n",
      "1400. reward: 0.011  policy: -0.036  value: 0.011  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "1500. reward: 0.011  policy: -0.030  value: 0.011  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "1600. reward: 0.011  policy: 0.059  value: 0.022  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "1700. reward: 0.011  policy: 0.033  value: 0.020  entropy: 2.079  confidence: 0.126  score: 1  vocab: 145\n",
      "1800. reward: 0.011  policy: 0.039  value: 0.021  entropy: 2.079  confidence: 0.125  score: 1  vocab: 145\n",
      "1900. reward: 0.011  policy: -0.062  value: 0.007  entropy: 2.079  confidence: 0.126  score: 1  vocab: 145\n",
      "2000. reward: 0.011  policy: -0.014  value: 0.014  entropy: 2.079  confidence: 0.124  score: 1  vocab: 145\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ef7c21980021>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"./games/rewardsDense_goalDetailed.ulx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trained in {:.2f} secs\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstarttime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-ab2666bf07f4>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(agent, path, max_step, nb_episodes, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mnb_moves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mnb_moves\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-a32b8d84e413>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, score, done, infos)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "agent = NeuralAgent()\n",
    "\n",
    "print(\"Training\")\n",
    "agent.train()\n",
    "starttime = time()\n",
    "play(agent,\"./games/rewardsDense_goalDetailed.ulx\", nb_episodes=500, verbose=False)\n",
    "print(\"Trained in {:.2f} secs\".format(time() - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "agent.test()\n",
    "play(agent, \"./games/rewardsDense_goalDetailed.ulx\")  # Dense rewards game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9111b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
