{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b28ce5",
   "metadata": {},
   "source": [
    "## Random Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88171026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textworld.gym\n",
    "\n",
    "\n",
    "class RandomAgent(textworld.gym.Agent):\n",
    "    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n",
    "    def __init__(self, seed=1234):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "\n",
    "    @property\n",
    "    def infos_to_request(self) -> textworld.EnvInfos:\n",
    "        return textworld.EnvInfos(admissible_commands=True)\n",
    "    \n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
    "        return self.rng.choice(infos[\"admissible_commands\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4b813",
   "metadata": {},
   "source": [
    "## Play Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5f70bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import gym\n",
    "import textworld.gym\n",
    "\n",
    "\n",
    "def play(agent, path, max_step=100, nb_episodes=10, verbose=True):\n",
    "    infos_to_request = agent.infos_to_request\n",
    "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
    "    \n",
    "    gamefiles = [path]\n",
    "    if os.path.isdir(path):\n",
    "        gamefiles = glob(os.path.join(path, \"*.ulx\"))\n",
    "        \n",
    "    env_id = textworld.gym.register_games(gamefiles,\n",
    "                                          request_infos=infos_to_request,\n",
    "                                          max_episode_steps=max_step)\n",
    "    env = gym.make(env_id)  # Create a Gym environment to play the text game.\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(os.path.dirname(path), end=\"\")\n",
    "        else:\n",
    "            print(os.path.basename(path), end=\"\")\n",
    "        \n",
    "    # Collect some statistics: nb_steps, final reward.\n",
    "    avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
    "    for no_episode in range(nb_episodes):\n",
    "        obs, infos = env.reset()  # Start new episode.\n",
    "\n",
    "        score = 0\n",
    "        done = False\n",
    "        nb_moves = 0\n",
    "        while not done:\n",
    "            command = agent.act(obs, score, done, infos)\n",
    "            obs, score, done, infos = env.step(command)\n",
    "            nb_moves += 1\n",
    "        \n",
    "        agent.act(obs, score, done, infos)  # Let the agent know the game is done.\n",
    "                \n",
    "        if verbose:\n",
    "            print(\".\", end=\"\")\n",
    "        avg_moves.append(nb_moves)\n",
    "        avg_scores.append(score)\n",
    "        avg_norm_scores.append(score / infos[\"max_score\"])\n",
    "\n",
    "    env.close()\n",
    "    msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n",
    "        else:\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42137a",
   "metadata": {},
   "source": [
    "## Testing the Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4878844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewardsDense_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  4.2 / 10.\n",
      "rewardsBalanced_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  0.7 / 4.\n",
      "rewardsSparse_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  0.0 / 1.\n"
     ]
    }
   ],
   "source": [
    "play(RandomAgent(),\"./games/rewardsDense_goalDetailed.ulx\")\n",
    "play(RandomAgent(), \"./games/rewardsBalanced_goalDetailed.ulx\")\n",
    "play(RandomAgent(), \"./games/rewardsSparse_goalDetailed.ulx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6198b1",
   "metadata": {},
   "source": [
    "## Neural Agent (with Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b6b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Mapping, Any, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textworld\n",
    "import textworld.gym\n",
    "from textworld import EnvInfos\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class CommandScorer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CommandScorer, self).__init__()\n",
    "        torch.manual_seed(42)  # For reproducibility\n",
    "        self.embedding    = nn.Embedding(input_size, hidden_size)\n",
    "        self.encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
    "        self.cmd_encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
    "        self.state_gru    = nn.GRU(hidden_size, hidden_size)\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.state_hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "        self.critic       = nn.Linear(hidden_size, 1)\n",
    "        self.att_cmd      = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, obs, commands, **kwargs):\n",
    "        input_length = obs.size(0)\n",
    "        batch_size = obs.size(1)\n",
    "        nb_cmds = commands.size(1)\n",
    "\n",
    "        embedded = self.embedding(obs)\n",
    "        encoder_output, encoder_hidden = self.encoder_gru(embedded)\n",
    "        state_output, state_hidden = self.state_gru(encoder_hidden, self.state_hidden)\n",
    "        self.state_hidden = state_hidden\n",
    "        value = self.critic(state_output)\n",
    "\n",
    "        # Attention network over the commands.\n",
    "        cmds_embedding = self.embedding.forward(commands)\n",
    "        _, cmds_encoding_last_states = self.cmd_encoder_gru.forward(cmds_embedding)  # 1 x cmds x hidden\n",
    "\n",
    "        # Same observed state for all commands.\n",
    "        cmd_selector_input = torch.stack([state_hidden] * nb_cmds, 2)  # 1 x batch x cmds x hidden\n",
    "\n",
    "        # Same command choices for the whole batch.\n",
    "        cmds_encoding_last_states = torch.stack([cmds_encoding_last_states] * batch_size, 1)  # 1 x batch x cmds x hidden\n",
    "\n",
    "        # Concatenate the observed state and command encodings.\n",
    "        cmd_selector_input = torch.cat([cmd_selector_input, cmds_encoding_last_states], dim=-1)\n",
    "\n",
    "        # Compute one score per command.\n",
    "        scores = F.relu(self.att_cmd(cmd_selector_input)).squeeze(-1)  # 1 x Batch x cmds\n",
    "\n",
    "        probs = F.softmax(scores, dim=2)  # 1 x Batch x cmds\n",
    "        index = probs[0].multinomial(num_samples=1).unsqueeze(0) # 1 x batch x indx\n",
    "        return scores, index, value\n",
    "\n",
    "    def reset_hidden(self, batch_size):\n",
    "        self.state_hidden = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class NeuralAgent:\n",
    "    \"\"\" Simple Neural Agent for playing TextWorld games. \"\"\"\n",
    "    MAX_VOCAB_SIZE = 1000\n",
    "    UPDATE_FREQUENCY = 10\n",
    "    LOG_FREQUENCY = 1000\n",
    "    GAMMA = 0.9\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._initialized = False\n",
    "        self._epsiode_has_started = False\n",
    "        self.id2word = [\"<PAD>\", \"<UNK>\"]\n",
    "        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n",
    "        \n",
    "        self.model = CommandScorer(input_size=self.MAX_VOCAB_SIZE, hidden_size=128)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), 0.00003)\n",
    "        \n",
    "        self.mode = \"test\"\n",
    "    \n",
    "    def train(self):\n",
    "        self.mode = \"train\"\n",
    "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "        self.transitions = []\n",
    "        self.model.reset_hidden(1)\n",
    "        self.last_score = 0\n",
    "        self.no_train_step = 0\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = \"test\"\n",
    "        self.model.reset_hidden(1)\n",
    "        \n",
    "    @property\n",
    "    def infos_to_request(self) -> EnvInfos:\n",
    "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
    "                        won=True, lost=True)\n",
    "    \n",
    "    def _get_word_id(self, word):\n",
    "        if word not in self.word2id:\n",
    "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
    "                return self.word2id[\"<UNK>\"]\n",
    "            \n",
    "            self.id2word.append(word)\n",
    "            self.word2id[word] = len(self.word2id)\n",
    "            \n",
    "        return self.word2id[word]\n",
    "            \n",
    "    def _tokenize(self, text):\n",
    "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
    "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
    "        word_ids = list(map(self._get_word_id, text.split()))\n",
    "        return word_ids\n",
    "\n",
    "    def _process(self, texts):\n",
    "        texts = list(map(self._tokenize, texts))\n",
    "        max_len = max(len(l) for l in texts)\n",
    "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            padded[i, :len(text)] = text\n",
    "\n",
    "        padded_tensor = torch.from_numpy(padded).type(torch.long).to(device)\n",
    "        padded_tensor = padded_tensor.permute(1, 0) # Batch x Seq => Seq x Batch\n",
    "        return padded_tensor\n",
    "      \n",
    "    def _discount_rewards(self, last_values):\n",
    "        returns, advantages = [], []\n",
    "        R = last_values.data\n",
    "        for t in reversed(range(len(self.transitions))):\n",
    "            rewards, _, _, values = self.transitions[t]\n",
    "            R = rewards + self.GAMMA * R\n",
    "            adv = R - values\n",
    "            returns.append(R)\n",
    "            advantages.append(adv)\n",
    "            \n",
    "        return returns[::-1], advantages[::-1]\n",
    "\n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> Optional[str]:\n",
    "        \n",
    "        # Build agent's observation: feedback + look + inventory.\n",
    "        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
    "        \n",
    "        # Tokenize and pad the input and the commands to chose from.\n",
    "        input_tensor = self._process([input_])\n",
    "        commands_tensor = self._process(infos[\"admissible_commands\"])\n",
    "        \n",
    "        # Get our next action and value prediction.\n",
    "        outputs, indexes, values = self.model(input_tensor, commands_tensor)\n",
    "        action = infos[\"admissible_commands\"][indexes[0]]\n",
    "        \n",
    "        if self.mode == \"test\":\n",
    "            if done:\n",
    "                self.model.reset_hidden(1)\n",
    "            return action\n",
    "        \n",
    "        self.no_train_step += 1\n",
    "        \n",
    "        if self.transitions:\n",
    "            reward = score - self.last_score  # Reward is the gain/loss in score.\n",
    "            self.last_score = score\n",
    "            if infos[\"won\"]:\n",
    "                reward += 100\n",
    "            if infos[\"lost\"]:\n",
    "                reward -= 100\n",
    "                \n",
    "            self.transitions[-1][0] = reward  # Update reward information.\n",
    "        \n",
    "        self.stats[\"max\"][\"score\"].append(score)\n",
    "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
    "            # Update model\n",
    "            returns, advantages = self._discount_rewards(values)\n",
    "            \n",
    "            loss = 0\n",
    "            for transition, ret, advantage in zip(self.transitions, returns, advantages):\n",
    "                reward, indexes_, outputs_, values_ = transition\n",
    "                \n",
    "                advantage        = advantage.detach() # Block gradients flow here.\n",
    "                probs            = F.softmax(outputs_, dim=2)\n",
    "                log_probs        = torch.log(probs)\n",
    "                log_action_probs = log_probs.gather(2, indexes_)\n",
    "                policy_loss      = (-log_action_probs * advantage).sum()\n",
    "                value_loss       = (.5 * (values_ - ret) ** 2.).sum()\n",
    "                entropy     = (-probs * log_probs).sum()\n",
    "                loss += policy_loss + 0.5 * value_loss - 0.1 * entropy\n",
    "                \n",
    "                self.stats[\"mean\"][\"reward\"].append(reward)\n",
    "                self.stats[\"mean\"][\"policy\"].append(policy_loss.item())\n",
    "                self.stats[\"mean\"][\"value\"].append(value_loss.item())\n",
    "                self.stats[\"mean\"][\"entropy\"].append(entropy.item())\n",
    "                self.stats[\"mean\"][\"confidence\"].append(torch.exp(log_action_probs).item())\n",
    "            \n",
    "            if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
    "                msg = \"{}. \".format(self.no_train_step)\n",
    "                msg += \"  \".join(\"{}: {:.3f}\".format(k, np.mean(v)) for k, v in self.stats[\"mean\"].items())\n",
    "                msg += \"  \" + \"  \".join(\"{}: {}\".format(k, np.max(v)) for k, v in self.stats[\"max\"].items())\n",
    "                msg += \"  vocab: {}\".format(len(self.id2word))\n",
    "                print(msg)\n",
    "                self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "            \n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), 40)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "            self.transitions = []\n",
    "            self.model.reset_hidden(1)\n",
    "        else:\n",
    "            # Keep information about transitions for Truncated Backpropagation Through Time.\n",
    "            self.transitions.append([None, indexes, outputs, values])  # Reward will be set on the next call\n",
    "        \n",
    "        if done:\n",
    "            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744c5a41",
   "metadata": {},
   "source": [
    "## Training the neural agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bca25a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewardsDense_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  4.8 / 10.\n",
      "rewardsBalanced_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  0.7 / 4.\n",
      "rewardsSparse_goalDetailed.ulx..........  \tavg. steps: 100.0; avg. score:  0.0 / 1.\n"
     ]
    }
   ],
   "source": [
    "agent = NeuralAgent()\n",
    "play(agent, \"./games/rewardsDense_goalDetailed.ulx\")\n",
    "play(RandomAgent(), \"./games/rewardsBalanced_goalDetailed.ulx\")\n",
    "play(RandomAgent(), \"./games/rewardsSparse_goalDetailed.ulx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ea3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "1000. reward: 0.043  policy: 0.277  value: 0.070  entropy: 2.358  confidence: 0.096  score: 8  vocab: 263\n",
      "2000. reward: -0.062  policy: -1.605  value: 21.004  entropy: 2.344  confidence: 0.098  score: 9  vocab: 307\n",
      "3000. reward: -0.054  policy: -1.383  value: 23.397  entropy: 2.426  confidence: 0.093  score: 9  vocab: 309\n",
      "4000. reward: 0.051  policy: 0.129  value: 0.118  entropy: 2.470  confidence: 0.089  score: 6  vocab: 310\n",
      "5000. reward: 0.049  policy: 0.042  value: 0.093  entropy: 2.424  confidence: 0.097  score: 7  vocab: 310\n",
      "6000. reward: -0.053  policy: -0.173  value: 5.477  entropy: 2.479  confidence: 0.089  score: 9  vocab: 312\n",
      "7000. reward: 0.054  policy: 0.053  value: 0.079  entropy: 2.422  confidence: 0.097  score: 7  vocab: 312\n",
      "8000. reward: 0.062  policy: 0.058  value: 0.110  entropy: 2.472  confidence: 0.092  score: 8  vocab: 312\n",
      "9000. reward: 0.052  policy: -0.047  value: 0.095  entropy: 2.437  confidence: 0.098  score: 6  vocab: 312\n",
      "10000. reward: 0.067  policy: 0.102  value: 0.138  entropy: 2.489  confidence: 0.094  score: 9  vocab: 312\n",
      "11000. reward: 0.067  policy: 0.078  value: 0.130  entropy: 2.513  confidence: 0.089  score: 9  vocab: 313\n",
      "12000. reward: 0.063  policy: 0.025  value: 0.105  entropy: 2.521  confidence: 0.091  score: 8  vocab: 313\n",
      "13000. reward: -0.048  policy: -0.194  value: 5.496  entropy: 2.400  confidence: 0.098  score: 9  vocab: 314\n",
      "14000. reward: -0.054  policy: -1.238  value: 20.583  entropy: 2.349  confidence: 0.108  score: 9  vocab: 315\n",
      "15000. reward: 0.058  policy: -0.024  value: 0.097  entropy: 2.407  confidence: 0.100  score: 8  vocab: 315\n",
      "16000. reward: -0.047  policy: -1.517  value: 24.998  entropy: 2.472  confidence: 0.096  score: 9  vocab: 316\n",
      "17000. reward: -0.144  policy: -1.476  value: 26.079  entropy: 2.487  confidence: 0.096  score: 9  vocab: 318\n",
      "18000. reward: 0.063  policy: -0.000  value: 0.121  entropy: 2.488  confidence: 0.096  score: 7  vocab: 318\n",
      "19000. reward: -0.039  policy: -0.892  value: 16.371  entropy: 2.406  confidence: 0.105  score: 9  vocab: 318\n",
      "20000. reward: -0.032  policy: -1.728  value: 25.144  entropy: 2.459  confidence: 0.102  score: 9  vocab: 320\n",
      "21000. reward: 0.070  policy: -0.019  value: 0.126  entropy: 2.535  confidence: 0.099  score: 9  vocab: 320\n",
      "22000. reward: 0.070  policy: -0.040  value: 0.117  entropy: 2.493  confidence: 0.099  score: 8  vocab: 320\n",
      "23000. reward: 0.079  policy: 0.083  value: 0.147  entropy: 2.508  confidence: 0.098  score: 9  vocab: 320\n",
      "24000. reward: -0.139  policy: -2.204  value: 36.811  entropy: 2.450  confidence: 0.111  score: 9  vocab: 322\n",
      "25000. reward: -0.343  policy: -3.464  value: 58.681  entropy: 2.418  confidence: 0.110  score: 9  vocab: 326\n",
      "26000. reward: -0.342  policy: -6.447  value: 94.865  entropy: 2.376  confidence: 0.113  score: 9  vocab: 329\n",
      "27000. reward: -0.456  policy: -4.511  value: 73.222  entropy: 2.334  confidence: 0.132  score: 9  vocab: 332\n",
      "28000. reward: -0.012  policy: -0.124  value: 5.503  entropy: 2.338  confidence: 0.133  score: 9  vocab: 332\n",
      "29000. reward: -0.121  policy: -2.324  value: 41.063  entropy: 2.413  confidence: 0.134  score: 9  vocab: 332\n",
      "30000. reward: -0.011  policy: -0.406  value: 9.708  entropy: 2.313  confidence: 0.144  score: 9  vocab: 333\n",
      "31000. reward: -0.226  policy: -1.113  value: 20.384  entropy: 2.317  confidence: 0.142  score: 9  vocab: 336\n",
      "32000. reward: -0.329  policy: -4.233  value: 71.518  entropy: 2.303  confidence: 0.146  score: 9  vocab: 338\n",
      "33000. reward: -0.650  policy: -7.989  value: 131.158  entropy: 2.230  confidence: 0.163  score: 9  vocab: 340\n",
      "34000. reward: -0.336  policy: -5.815  value: 85.004  entropy: 2.325  confidence: 0.148  score: 9  vocab: 341\n",
      "35000. reward: -0.112  policy: -2.007  value: 34.218  entropy: 2.237  confidence: 0.157  score: 9  vocab: 342\n",
      "36000. reward: -0.316  policy: -3.195  value: 52.247  entropy: 2.231  confidence: 0.165  score: 9  vocab: 347\n",
      "37000. reward: -0.226  policy: -4.494  value: 64.802  entropy: 2.268  confidence: 0.173  score: 9  vocab: 348\n",
      "38000. reward: -0.012  policy: 0.809  value: 39.765  entropy: 2.209  confidence: 0.177  score: 10  vocab: 350\n",
      "39000. reward: -0.324  policy: -3.793  value: 64.148  entropy: 2.201  confidence: 0.177  score: 9  vocab: 353\n",
      "40000. reward: -0.231  policy: -2.346  value: 47.433  entropy: 2.231  confidence: 0.162  score: 9  vocab: 354\n",
      "41000. reward: -0.329  policy: -3.924  value: 67.514  entropy: 2.233  confidence: 0.160  score: 9  vocab: 356\n",
      "42000. reward: -0.537  policy: -5.627  value: 83.540  entropy: 2.232  confidence: 0.170  score: 9  vocab: 358\n",
      "43000. reward: -0.012  policy: -1.667  value: 21.804  entropy: 2.238  confidence: 0.161  score: 9  vocab: 358\n",
      "44000. reward: -0.214  policy: -3.437  value: 58.637  entropy: 2.286  confidence: 0.162  score: 9  vocab: 359\n",
      "45000. reward: -0.009  policy: -1.740  value: 24.945  entropy: 2.256  confidence: 0.160  score: 9  vocab: 360\n",
      "46000. reward: -0.113  policy: -2.491  value: 38.944  entropy: 2.308  confidence: 0.151  score: 9  vocab: 361\n",
      "47000. reward: 0.120  policy: -0.065  value: 93.081  entropy: 2.250  confidence: 0.157  score: 10  vocab: 362\n",
      "Trained in 2410.58 secs\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "agent = NeuralAgent()\n",
    "\n",
    "print(\"Training\")\n",
    "agent.train()  # Tell the agent it should update its parameters.\n",
    "starttime = time()\n",
    "play(agent, \"./games/rewardsDense_goalDetailed.ulx\", nb_episodes=500, verbose=False)  # Dense rewards game.\n",
    "print(\"Trained in {:.2f} secs\".format(time() - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7cc9bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewardsDense_goalDetailed.ulx..........  \tavg. steps:  85.4; avg. score:  9.0 / 10.\n"
     ]
    }
   ],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "agent.test()\n",
    "play(agent, \"./games/rewardsDense_goalDetailed.ulx\")  # Dense rewards game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c9111b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed: 1\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/games/another_game.ulx\n",
      "\n",
      "Objective:\n",
      "Hey, thanks for coming over to the TextWorld today, there is something I need you to do for me. First of all, you could, like, look and see that the antique trunk inside the bedroom is opened. Then, recover the old key from the antique trunk. Then, make absolutely sure that the wooden door inside the bedroom is unlocked. After unlocking the wooden door, open the wooden door in the bedroom. Then, try to head east. After that, try to travel south. Once you get through with that, take the milk from the couch within the living room. Having taken the milk, attempt to travel north. That done, rest the milk on the stove inside the kitchen. And if you do that, you're the winner!\n",
      "\n",
      "Walkthrough:\n",
      "open antique trunk > take old key from antique trunk > unlock wooden door with old key > open wooden door > go east > go south > take milk from couch > go north > put milk on stove\n",
      "\n",
      "-= Stats =-\n",
      "Nb. locations: 6\n",
      "Nb. objects: 28\n"
     ]
    }
   ],
   "source": [
    "!tw-make tw-simple --rewards dense --goal detailed --seed 1 --output games/another_game.ulx -v -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1e78196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another_game.ulx..........  \tavg. steps: 100.0; avg. score:  3.9 / 8.\n",
      "another_game.ulx..........  \tavg. steps:  93.2; avg. score:  6.5 / 8.\n"
     ]
    }
   ],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "play(RandomAgent(), \"./games/another_game.ulx\")\n",
    "play(agent, \"./games/another_game.ulx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba8ba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed: 3\n",
      "Global seed: 1\n",
      "Global seed: 4\n",
      "Global seed: 2\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-ekDZtbGXIbO5FKp8.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-E5eLHkaXFk6BSgR1.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-ek06H8B7uqoYFVEy.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-D8gMTlO8cPoEtgZx.ulx\n",
      "Global seed: 5\n",
      "Global seed: 6\n",
      "Global seed: 7\n",
      "Global seed: 8\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-7KpYUDDdckE0cBqZ.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-68kvf8x7TBd9Iq0P.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-o2RVTmrEi6R5T3p0.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-KJODI168SvJVFM9x.ulx\n",
      "Global seed: 9\n",
      "Global seed: 10\n",
      "Global seed: 11\n",
      "Global seed: 12\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-NPQ8TkJ9i2x6fYDM.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-redEHVr6CmKYhrJg.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-Q9nDu630U5j3tqBG.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-Qbq3h3VWFkPdtogB.ulx\n",
      "Global seed: 13\n",
      "Global seed: 14\n",
      "Global seed: 15\n",
      "Global seed: 16\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-jROVIEqEIya6Tr0L.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-1QKVfg6YhRb1ul1e.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-7yGrcV9pTE8DF75n.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-6GMVtjVYF5QRupyN.ulx\n",
      "Global seed: 17\n",
      "Global seed: 18\n",
      "Global seed: 19\n",
      "Global seed: 20\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-Mn8oTkr2fvv8TX1.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-nrEoHEqgUba7U1nx.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-VLpEiW2msKJpTZ7J.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-o8WVtobyuRR0Uqv5.ulx\n",
      "Global seed: 21\n",
      "Global seed: 23\n",
      "Global seed: 22\n",
      "Global seed: 24\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-JGJdTBvVHrpBiVy5.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-6GZ3CqJvCX9rfev3.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-M7MmCGG5i6kES1kV.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-1MB8fmosEL9HNEv.ulx\n",
      "Global seed: 25\n",
      "Global seed: 26\n",
      "Global seed: 27\n",
      "Global seed: 28\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-mn3JuMrnfPNNI5K5.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-pGedtxVJsYDxsGaV.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-P6RdC912uMrbcXBX.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-R2bxU9dJUK0LCxk0.ulx\n",
      "Global seed: 29\n",
      "Global seed: 30\n",
      "Global seed: 31\n",
      "Global seed: 32\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-W1qJibX5FqLRS3kL.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-6WNBiZyofr8mu6MG.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-eDi7Z2iEJ7FdL9.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-QlDlS6DxCMnVu81D.ulx\n",
      "Global seed: 33\n",
      "Global seed: 34\n",
      "Global seed: 35\n",
      "Global seed: 36\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-5rjrFkZEiyOIj1y.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-dZ2oU2KnflPksnEY.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-KrNpTrdMtdqVUKOB.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-J9ylcQmmc190Cgn3.ulx\n",
      "Global seed: 37\n",
      "Global seed: 38\n",
      "Global seed: 39\n",
      "Global seed: 40\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-M8xnUOBkulX7HNQX.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-nOVQCNlrINVxIDje.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-535aCDxgu5MqF7R1.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-mDjNimx7S5a5tmZ7.ulx\n",
      "Global seed: 41\n",
      "Global seed: 42\n",
      "Global seed: 43\n",
      "Global seed: 44\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-566vUvgjfXgU9GK.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-67XKC3EyH2riOk8.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-El9oUXJWIYVgF1jy.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-mbJLuQBoCbbkUv1n.ulx\n",
      "Global seed: 45\n",
      "Global seed: 46\n",
      "Global seed: 47\n",
      "Global seed: 48\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-M2vecROLFVBxHBvl.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-mNB2tM2ohGmRIB2J.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-JBP7TQZ6fV7biZ9q.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-aernSYY1f7rrflvB.ulx\n",
      "Global seed: 49\n",
      "Global seed: 50\n",
      "Global seed: 51\n",
      "Global seed: 52\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-5VVVSK6fGjlspnj.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-Oq9ns8K0uK5EhJvr.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-Mj8NTxYWso7LfmN9.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-xqa9SlmxsDmVs8yb.ulx\n",
      "Global seed: 53\n",
      "Global seed: 54\n",
      "Global seed: 55\n",
      "Global seed: 56\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-GN1YHrQ6s6vBTnNb.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-EloyS7BZs8LRHNVn.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-XR5RUnmMIgOnCYyy.ulx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-p1RLFX7MU7KjFy0d.ulx\n",
      "Global seed: 57\n",
      "Global seed: 58\n",
      "Global seed: 59\n",
      "Global seed: 60\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-WxnkhG07upKRhbNV.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-Kx32iybbtDGRFQl6.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-DWvMiBQvCR5sNkx.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-R7y2UJkBUeEqS8Bk.ulx\n",
      "Global seed: 61\n",
      "Global seed: 62\n",
      "Global seed: 63\n",
      "Global seed: 64\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-YnvLs6vYIWvWC51e.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-1EQPheDOiVB8I7m5.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-6VXXCVGKiEWkhPer.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-G6a7HeZPcJNGhLQ2.ulx\n",
      "Global seed: 65\n",
      "Global seed: 66\n",
      "Global seed: 67\n",
      "Global seed: 68\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-1jr7FeK9TgZCQ9n.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-0a72tMGVtQnPSvMR.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-2OOjSlaNUBKcgLy.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-1vbrCekqT6xMcMdP.ulx\n",
      "Global seed: 69\n",
      "Global seed: 70\n",
      "Global seed: 71\n",
      "Global seed: 72\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-6kRniM7gfXQNCyj6.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-kGDLudo8fXeJH1Yg.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-RvQYhbdKfMyqS5Wp.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-aMq9U18aFMlOHVEe.ulx\n",
      "Global seed: 73\n",
      "Global seed: 74\n",
      "Global seed: 75\n",
      "Global seed: 76\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-8koWh6KqcgNPI8Ox.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-165ai83atZWatMRa.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-Eykru3m8ilOGiBnx.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-vKm0IdN8c0VRfJl1.ulx\n",
      "Global seed: 77\n",
      "Global seed: 78\n",
      "Global seed: 79\n",
      "Global seed: 80\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-D3PEFjpNIQykuqra.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-gPyQi5vkhdQOUV0J.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-jYPJukgls6oZf6qG.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-eRNBC9q7tr5Vi3Mv.ulx\n",
      "Global seed: 81\n",
      "Global seed: 82\n",
      "Global seed: 83\n",
      "Global seed: 84\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-0lvotxEKtYZ6umyJ.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-3bj9creDs3V0IKKN.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-qZDfWDNTJmQiLvq.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-6yPJsEJMTeR6fpPR.ulx\n",
      "Global seed: 85\n",
      "Global seed: 86\n",
      "Global seed: 87\n",
      "Global seed: 88\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-WrmvTdgGc05rh3eZ.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-g0GEcMQ5CWaeF3nJ.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-5PBeh9J3IX5XUjL3.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-DvPBs6OphW6nCnQ3.ulx\n",
      "Global seed: 89\n",
      "Global seed: 90\n",
      "Global seed: 91\n",
      "Global seed: 92\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-eBxGu3KeiYjbHDl9.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-XElPhYRjT8gWs1eM.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-3qLvIOZ9c66Igby.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-OWRZsvM8h6XyIpQy.ulx\n",
      "Global seed: 93\n",
      "Global seed: 94\n",
      "Global seed: 95\n",
      "Global seed: 96\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-gkdXTMLPtKpeSY5J.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-n3OYtnV7IY8rTkBq.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-q3rdiVK5I1lrsNpj.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-WvgBsoOmhDObfRgL.ulx\n",
      "Global seed: 97\n",
      "Global seed: 98\n",
      "Global seed: 99\n",
      "Global seed: 100\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-xQNoFqlDs10nfr79.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-1dgPTd1ki5kQUkrn.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-o7PNc5j0URa5HYLb.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/training_games/tw-simple-rDense+gDetailed+train-house-GP-2KK5iXD8ueb3ikl6.ulx\n"
     ]
    }
   ],
   "source": [
    "! seq 1 100 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --format ulx --output training_games/ --seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b382de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 100 games\n",
      "1000. reward: 0.172  policy: 1.946  value: 19.621  entropy: 2.416  confidence: 0.092  score: 10  vocab: 486\n",
      "2000. reward: 0.046  policy: 0.077  value: 0.070  entropy: 2.404  confidence: 0.094  score: 5  vocab: 554\n",
      "3000. reward: 0.036  policy: 0.062  value: 0.060  entropy: 2.344  confidence: 0.099  score: 6  vocab: 597\n",
      "4000. reward: 0.054  policy: 0.157  value: 0.108  entropy: 2.485  confidence: 0.088  score: 7  vocab: 615\n",
      "5000. reward: 0.047  policy: 0.024  value: 0.086  entropy: 2.370  confidence: 0.099  score: 6  vocab: 631\n",
      "6000. reward: -0.051  policy: -1.764  value: 25.028  entropy: 2.494  confidence: 0.088  score: 7  vocab: 658\n",
      "7000. reward: 0.050  policy: -0.036  value: 0.085  entropy: 2.374  confidence: 0.098  score: 6  vocab: 659\n",
      "8000. reward: 0.054  policy: 0.025  value: 0.079  entropy: 2.405  confidence: 0.099  score: 9  vocab: 663\n",
      "9000. reward: 0.048  policy: -0.037  value: 0.076  entropy: 2.383  confidence: 0.099  score: 5  vocab: 670\n",
      "10000. reward: 0.053  policy: 0.067  value: 0.104  entropy: 2.406  confidence: 0.098  score: 8  vocab: 691\n",
      "11000. reward: 0.051  policy: 0.028  value: 0.089  entropy: 2.426  confidence: 0.098  score: 8  vocab: 691\n",
      "12000. reward: 0.054  policy: 0.070  value: 0.104  entropy: 2.445  confidence: 0.097  score: 6  vocab: 692\n",
      "13000. reward: -0.054  policy: -1.428  value: 20.757  entropy: 2.427  confidence: 0.097  score: 8  vocab: 695\n",
      "14000. reward: 0.057  policy: 0.086  value: 0.121  entropy: 2.453  confidence: 0.097  score: 6  vocab: 695\n",
      "15000. reward: -0.063  policy: -0.683  value: 9.821  entropy: 2.434  confidence: 0.096  score: 6  vocab: 699\n",
      "16000. reward: 0.049  policy: 0.012  value: 0.095  entropy: 2.414  confidence: 0.099  score: 7  vocab: 699\n",
      "17000. reward: 0.061  policy: 0.025  value: 0.095  entropy: 2.484  confidence: 0.094  score: 9  vocab: 700\n",
      "18000. reward: 0.054  policy: 0.025  value: 0.099  entropy: 2.384  confidence: 0.102  score: 6  vocab: 700\n",
      "19000. reward: 0.054  policy: -0.066  value: 0.097  entropy: 2.426  confidence: 0.102  score: 7  vocab: 700\n",
      "20000. reward: -0.044  policy: -1.137  value: 20.738  entropy: 2.430  confidence: 0.099  score: 9  vocab: 704\n",
      "21000. reward: 0.063  policy: 0.046  value: 0.130  entropy: 2.529  confidence: 0.093  score: 9  vocab: 704\n",
      "22000. reward: 0.063  policy: 0.063  value: 0.112  entropy: 2.495  confidence: 0.096  score: 9  vocab: 708\n",
      "23000. reward: 0.060  policy: -0.032  value: 0.101  entropy: 2.457  confidence: 0.103  score: 9  vocab: 708\n",
      "24000. reward: 0.063  policy: -0.057  value: 0.099  entropy: 2.413  confidence: 0.103  score: 9  vocab: 708\n",
      "25000. reward: -0.138  policy: -2.706  value: 41.471  entropy: 2.480  confidence: 0.098  score: 9  vocab: 715\n",
      "26000. reward: 0.067  policy: -0.001  value: 0.144  entropy: 2.466  confidence: 0.107  score: 9  vocab: 717\n",
      "27000. reward: 0.071  policy: -0.156  value: 32.850  entropy: 2.464  confidence: 0.107  score: 9  vocab: 719\n",
      "28000. reward: 0.083  policy: 1.024  value: 25.375  entropy: 2.480  confidence: 0.106  score: 9  vocab: 723\n",
      "29000. reward: 0.184  policy: 1.535  value: 21.644  entropy: 2.465  confidence: 0.109  score: 9  vocab: 725\n",
      "30000. reward: -0.040  policy: -0.529  value: 9.949  entropy: 2.524  confidence: 0.106  score: 9  vocab: 727\n",
      "31000. reward: -0.134  policy: -1.786  value: 29.136  entropy: 2.449  confidence: 0.112  score: 9  vocab: 729\n",
      "32000. reward: -0.149  policy: -1.549  value: 25.866  entropy: 2.451  confidence: 0.117  score: 9  vocab: 731\n",
      "33000. reward: 0.084  policy: 0.328  value: 31.213  entropy: 2.438  confidence: 0.116  score: 9  vocab: 735\n",
      "34000. reward: -0.027  policy: -0.801  value: 13.367  entropy: 2.351  confidence: 0.128  score: 9  vocab: 735\n",
      "35000. reward: 0.073  policy: -0.011  value: 0.146  entropy: 2.412  confidence: 0.120  score: 9  vocab: 736\n",
      "36000. reward: -0.140  policy: -3.043  value: 41.904  entropy: 2.458  confidence: 0.115  score: 9  vocab: 738\n",
      "37000. reward: -0.047  policy: -1.257  value: 18.300  entropy: 2.417  confidence: 0.116  score: 9  vocab: 746\n",
      "38000. reward: 0.199  policy: 0.669  value: 10.926  entropy: 2.461  confidence: 0.117  score: 10  vocab: 746\n",
      "39000. reward: -0.040  policy: -0.621  value: 9.837  entropy: 2.508  confidence: 0.107  score: 9  vocab: 747\n",
      "40000. reward: -0.026  policy: -1.444  value: 25.177  entropy: 2.428  confidence: 0.120  score: 9  vocab: 748\n",
      "41000. reward: 0.174  policy: 1.783  value: 25.641  entropy: 2.533  confidence: 0.108  score: 7  vocab: 749\n",
      "42000. reward: -0.452  policy: -6.499  value: 98.036  entropy: 2.413  confidence: 0.123  score: 9  vocab: 751\n",
      "43000. reward: -0.244  policy: -2.719  value: 50.190  entropy: 2.367  confidence: 0.140  score: 9  vocab: 751\n",
      "44000. reward: -0.454  policy: -5.661  value: 84.788  entropy: 2.468  confidence: 0.123  score: 9  vocab: 756\n",
      "45000. reward: -0.466  policy: -5.757  value: 86.746  entropy: 2.459  confidence: 0.117  score: 9  vocab: 756\n",
      "46000. reward: -0.138  policy: -2.799  value: 40.408  entropy: 2.363  confidence: 0.130  score: 9  vocab: 756\n",
      "47000. reward: -0.247  policy: -3.346  value: 57.436  entropy: 2.446  confidence: 0.124  score: 9  vocab: 758\n",
      "48000. reward: 0.091  policy: 0.390  value: 40.652  entropy: 2.379  confidence: 0.130  score: 9  vocab: 760\n",
      "Trained in 2325.98 secs\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "agent = NeuralAgent()\n",
    "\n",
    "print(\"Training on 100 games\")\n",
    "agent.train()  # Tell the agent it should update its parameters.\n",
    "starttime = time()\n",
    "play(agent, \"./training_games/\", nb_episodes=100 * 5, verbose=False)  # Each game will be seen 5 times.\n",
    "print(\"Trained in {:.2f} secs\".format(time() - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d1af7",
   "metadata": {},
   "source": [
    "### Evaluating agent on 20 test games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0c012ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed: 3\n",
      "Global seed: 2\n",
      "Global seed: 1\n",
      "Global seed: 4\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-E5eLHkaXFk6BSgR1.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-D8gMTlO8cPoEtgZx.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-ekDZtbGXIbO5FKp8.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-ek06H8B7uqoYFVEy.ulx\n",
      "Global seed: 5\n",
      "Global seed: 6\n",
      "Global seed: 7\n",
      "Global seed: 8\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-7KpYUDDdckE0cBqZ.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-o2RVTmrEi6R5T3p0.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-68kvf8x7TBd9Iq0P.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-KJODI168SvJVFM9x.ulx\n",
      "Global seed: 9\n",
      "Global seed: 10\n",
      "Global seed: 11\n",
      "Global seed: 12\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-redEHVr6CmKYhrJg.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-NPQ8TkJ9i2x6fYDM.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-Q9nDu630U5j3tqBG.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-Qbq3h3VWFkPdtogB.ulx\n",
      "Global seed: 13\n",
      "Global seed: 14\n",
      "Global seed: 15\n",
      "Global seed: 16\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-jROVIEqEIya6Tr0L.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-1QKVfg6YhRb1ul1e.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-7yGrcV9pTE8DF75n.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-6GMVtjVYF5QRupyN.ulx\n",
      "Global seed: 17\n",
      "Global seed: 19\n",
      "Global seed: 18\n",
      "Global seed: 20\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-nrEoHEqgUba7U1nx.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-VLpEiW2msKJpTZ7J.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-Mn8oTkr2fvv8TX1.ulx\n",
      "Game generated: /home/prasath/TextWorld-master/notebooks/testing_games/tw-simple-rDense+gDetailed+test-house-GP-o8WVtobyuRR0Uqv5.ulx\n"
     ]
    }
   ],
   "source": [
    "! seq 1 20 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --test --format ulx --output testing_games/ --seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8d97bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewardsDense_goalDetailed.ulx..........  \tavg. steps:  83.4; avg. score:  8.5 / 10.\n",
      "./testing_games........................................................................................................................................................................................................  \tavg. steps:  89.9; avg. score:  0.8 / 1.\n",
      "./testing_games........................................................................................................................................................................................................  \tavg. steps:  99.3; avg. score:  0.5 / 1.\n"
     ]
    }
   ],
   "source": [
    "agent.test()\n",
    "play(agent, \"./games/rewardsDense_goalDetailed.ulx\")  # Averaged over 10 playthroughs.\n",
    "play(agent, \"./testing_games/\", nb_episodes=20 * 10)  # Averaged over 10 playthroughs for each test game.\n",
    "play(RandomAgent(), \"./testing_games/\", nb_episodes=20 * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf695961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
